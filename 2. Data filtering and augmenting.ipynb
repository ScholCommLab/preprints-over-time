{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last updated: July 5, 2023\n",
    "\n",
    "Last run: June 2, 2023\n",
    "\n",
    "**Data filtering and augmenting with dates from Crossref and ArXiv**\n",
    "\n",
    "## Unreviewed science in the news: The evolution of preprint media coverage from 2014-2021\n",
    "\n",
    "\n",
    "Juan Pablo Alperin\n",
    "\n",
    "**Related Publication:**\n",
    "Fleerackers, A., Shores, K., Chtena, N. & Alperin, J.P. (2023). Unreviewed science in the news: The evolution of preprint media coverage from 2014-2021. *bioarxiv*. \n",
    "\n",
    "**Related Dataset:**\n",
    "Alperin, Juan Pablo; Fleerackers; Shores, 2023, \"Data for: Unreviewed science in the news\", https://doi.org/10.7910/DVN/ZHQUFD, *Harvard Dataverse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Some of the code below may not run perfectly. Accessing the APIs would sometimes timeout or stop and so there was some amount of retarting and merging that was necessary at the time. The below should be the final working version of the code, but your mileage may vary if you try to re-use as-is. Also note that API responses will change on future runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from habanero import Crossref\n",
    "cr = Crossref()\n",
    "\n",
    "import arxiv\n",
    "arxiv.Client(\n",
    "  page_size = 100,\n",
    "  delay_seconds = 3,\n",
    "  num_retries = 3\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import json\n",
    "\n",
    "RETRY_QUERIES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/top outlets/preprints_mention_details.csv'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crossref(doi):\n",
    "    try:\n",
    "        r = cr.works(ids = doi)\n",
    "        return json.dumps(r['message'])\n",
    "    except KeyboardInterrupt: \n",
    "        raise\n",
    "    except Exception as err:\n",
    "        print(doi, type(err).__name__)\n",
    "        return type(err).__name__\n",
    "    \n",
    "def get_arxiv(arxiv_id):\n",
    "    try:\n",
    "        search = arxiv.Search(id_list=[arxiv_id])\n",
    "        paper = next(search.results())\n",
    "\n",
    "        ret = {}\n",
    "        ret['arxiv_title'] = paper.title\n",
    "        ret['arxiv_published_date'] = paper.published\n",
    "        ret['arxiv_update_date'] = paper.updated\n",
    "        ret['doi'] = paper.doi\n",
    "        return ret\n",
    "    except KeyboardInterrupt: \n",
    "        raise\n",
    "    except Exception as err:\n",
    "        print(arxiv_id, type(err).__name__)\n",
    "        return type(err).__name__     \n",
    "    \n",
    "def parse_crossref(cr):\n",
    "    ret = {}\n",
    "    \n",
    "    try:\n",
    "        ret['crossref_create_date'] = cr['created']['date-time']\n",
    "    except:\n",
    "        raise\n",
    "    \n",
    "    try: \n",
    "        publish_date = cr['published']['date-parts'][0]\n",
    "        assert len(publish_date) == 3\n",
    "        ret['crossref_publish_date'] = '-'.join(map(str,publish_date))\n",
    "    except: \n",
    "        ret['crossref_publish_date'] = None\n",
    "    \n",
    "    try:\n",
    "        ret['crossref_title'] = cr['title'][0]\n",
    "    except:\n",
    "        ret['crossref_title'] = None\n",
    "        \n",
    "    try: \n",
    "        ret['crossref_published_doi'] = cr['relation']['is-preprint-of'][0]['id']\n",
    "    except:\n",
    "        ret['crossref_published_doi'] = None\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## API responses folder is not\n",
    "try:\n",
    "    arxivs = pd.read_csv('data/top outlets/API responses/arxivs_full.csv', index_col='arxiv_id')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "if RETRY_QUERIES: \n",
    "    arxiv_ids = df[df.arxiv_id.notnull()].arxiv_id.drop_duplicates().to_list()\n",
    "    # arxiv_ids.loc[:,'archive_response'] = arxiv_ids.arxiv_id.progress_apply(get_arxiv)\n",
    "    try: \n",
    "        if arxivs.shape[0] > 0:\n",
    "            arxiv_ids = list(set(arxiv_ids).difference(arxivs.index))\n",
    "    except:\n",
    "        arxivs = pd.DataFrame()\n",
    "\n",
    "    for arxiv_id in tqdm(arxiv_ids):\n",
    "        paper =  get_arxiv(arxiv_id)\n",
    "        arxivs = arxivs.append(pd.DataFrame(paper, index=[arxiv_id]))\n",
    "        time.sleep(3)\n",
    "    \n",
    "    arxivs.to_csv('data/top outlets/arxivs_full.csv')\n",
    "\n",
    "arxivs.rename(columns={'doi': 'arxiv_published_doi'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_ids\n",
    "df = df.merge(arxivs, how='left', left_on='arxiv_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    arxiv_dois = pd.read_csv('data/top outlets/API responses/arxiv_doi_responses.csv', index_col='arxiv_id')\n",
    "except: \n",
    "    arxiv_dois = arxivs[arxivs.arxiv_published_doi.notnull()].arxiv_published_doi.drop_duplicates().to_frame()\n",
    "    arxiv_dois.loc[:,'crossref_response'] = arxiv_dois.arxiv_published_doi.progress_apply(get_crossref)\n",
    "    arxiv_dois.to_csv('data/top outlets/arxiv_doi_responses.csv')\n",
    "\n",
    "arxiv_dois = arxiv_dois[arxiv_dois.crossref_response != '\"JSONDecodeError\"']\n",
    "arxiv_dois = arxiv_dois[arxiv_dois.crossref_response != '\"TypeError\"']\n",
    "arxiv_dois.loc[:,'crossref_response'] = arxiv_dois.crossref_response.map(json.loads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if c.startswith('arxiv_published_doi_'):\n",
    "        del df[c]\n",
    "\n",
    "df2 = arxiv_dois.crossref_response.map(parse_crossref)\n",
    "df2 = pd.json_normalize(df2).set_index(df2.index)\n",
    "del df2['crossref_published_doi']  # these are the DOIs from arxiv, so shouldn't be a preprint of anything else\n",
    "df2.columns = ['arxiv_published_doi_%s' % s for s in df2.columns]\n",
    "\n",
    "df = df.merge(df2, how=\"left\", left_on='arxiv_id', right_index=True)\n",
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    dois = pd.read_csv('data/top outlets/API responses/crossref_responses.csv', index_col='doi')\n",
    "except:\n",
    "    dois = df[df.doi.notnull()].doi.drop_duplicates().to_frame()\n",
    "    dois.loc[:,'crossref_response'] = dois.doi.progress_apply(get_crossref)  \n",
    "    dois.to_csv('data/top outlets/crossref_responses.csv')    \n",
    "\n",
    "if RETRY_QUERIES: \n",
    "    dois.reset_index(inplace=True)\n",
    "    dois.loc[dois.crossref_response == 'JSONDecodeError','crossref_response'] = dois.loc[dois.crossref_response == 'JSONDecodeError', 'doi'].progress_apply(get_crossref)\n",
    "    dois.set_index('doi', inplace=True)\n",
    "    dois.to_csv('data/top outlets/crossref_responses.csv')    \n",
    "\n",
    "dois = dois[dois.crossref_response != 'JSONDecodeError']\n",
    "dois['crossref_response'] = dois.crossref_response.map(json.loads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if c.startswith('crossref'):\n",
    "        del df[c]\n",
    "\n",
    "# add in the DOI error messages\n",
    "df.merge(dois, how='left', left_on = 'doi', right_index=True)\n",
    "\n",
    "# add in the DOI fields\n",
    "df2 = dois[dois.crossref_response.map(type) != str].crossref_response.map(parse_crossref)\n",
    "df2 = pd.json_normalize(df2).set_index(df2.index)\n",
    "df = df.merge(df2, how='left', left_on='doi', right_index=True)\n",
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    cr_published_dois = pd.read_csv('data/top outlets/API responses/cr_published_doi_responses.csv', index_col='crossref_published_doi')\n",
    "except:\n",
    "    cr_published_dois = df[df.crossref_published_doi.notnull()].crossref_published_doi.drop_duplicates().to_frame()\n",
    "    cr_published_dois.loc[:,'crossref_response'] = cr_published_dois.crossref_published_doi.progress_apply(get_crossref)  \n",
    "    cr_published_dois.set_index('crossref_published_doi', inplace=True)\n",
    "    cr_published_dois.to_csv('data/top outlets/cr_published_doi_responses.csv')    \n",
    "\n",
    "cr_published_dois = cr_published_dois[cr_published_dois.crossref_response != 'JSONDecodeError']\n",
    "cr_published_dois['crossref_response'] = cr_published_dois.crossref_response.map(json.loads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    if c.startswith('crossref_published_doi_'):\n",
    "        del df[c]\n",
    "\n",
    "df2 = cr_published_dois.crossref_response.map(parse_crossref)\n",
    "df2 = pd.json_normalize(df2).set_index(df2.index)\n",
    "del df2['crossref_published_doi']  # these are the DOIs from arxiv, so shouldn't be a preprint of anything else\n",
    "df2.columns = ['crossref_published_doi_%s' % s[9:] for s in df2.columns]\n",
    "\n",
    "df = df.merge(df2, how=\"left\", left_on='crossref_published_doi', right_index=True)\n",
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def best_published_date(row):\n",
    "dates = ['first_seen_on', 'posted_on'] + [c for c in df.columns if c[-4:] == 'date']\n",
    "\n",
    "for d in dates:\n",
    "    df.loc[:,d] = pd.to_datetime(df[d], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code below assigns the best guess as to when a preprint was published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_preprint_pub_date(row):\n",
    "    dates = [x for x in [row['arxiv_published_date'], row['crossref_create_date'], row['pubdate']] if pd.notnull(x)]\n",
    "\n",
    "    if len(dates) > 0:    \n",
    "        if row['server'] == 'ssrn': \n",
    "            return min([x for x in [row['crossref_create_date'], row['pubdate']] if pd.notnull(x)])\n",
    "\n",
    "        if row['server'] == 'arxiv':\n",
    "            return row['arxiv_published_date']\n",
    "                       \n",
    "        if row['server'] in ('biorxiv', 'medrxiv'):\n",
    "            return min([x for x in [row['crossref_create_date'], row['crossref_publish_date']] if pd.notnull(x)])\n",
    "    \n",
    "    else:\n",
    "        # only use this as a last resort\n",
    "        return row['first_seen_on']\n",
    "\n",
    "# timezone fixing\n",
    "for d in dates:\n",
    "    df[d] = pd.to_datetime(df[d] ,utc=True)\n",
    "    \n",
    "df.loc[:,'best_preprint_pub_date'] = df.apply(best_preprint_pub_date, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code below assigns the best guess as to when the peer-reviewed version was published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_published_pub_date(row):\n",
    "    if pd.notnull(raow['arxiv_published_doi_crossref_create_date']):\n",
    "        return row['arxiv_published_doi_crossref_create_date']\n",
    "    elif pd.notnull(row['crossref_published_doi_create_date']):\n",
    "        return row['crossref_published_doi_create_date']\n",
    "    dates = [x for x in [row['arxiv_published_doi_crossref_create_date'], row['crossref_published_doi_create_date']] if pd.notnull(x)]\n",
    "    if len(dates) > 0: \n",
    "        return min(dates)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df.loc[:,'best_published_pub_date'] = df.apply(best_published_pub_date, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'days_since_preprint'] = (df.posted_on - df.best_preprint_pub_date).map(lambda x: x.days)\n",
    "df.loc[:,'days_since_publication'] = (df.posted_on - df.best_published_pub_date).map(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/top outlets/preprints_mention_details_with_dates.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
